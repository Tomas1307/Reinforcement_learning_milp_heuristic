"""
Implementaci√≥n del algoritmo Scatter Search principal
"""
import json
import random
import numpy as np
from typing import Dict, List, Tuple, Any
import time
from datetime import datetime
import yaml
from .hyperparameter_space import HyperparameterSpace
from .cosine_similarity_hyperparams import CosineSimilarityCalculator
from ..dqn_agent.training import train_dqn_agent
from ..dqn_agent.agent import EnhancedDQNAgentPyTorch
from ..dqn_agent.environment import EVChargingEnv
import os
class ScatterSearchOptimizer:
    """Implementaci√≥n principal del algoritmo Scatter Search"""
    
    def __init__(self, config_path: str, systems_data: Dict, output_dir: str = None):
        self.load_config(config_path)
        self.systems_data = systems_data
        
        self.hyperparameter_space = HyperparameterSpace(
            "./src/configs/hyperparameter_ranges.yaml"
        )
        self.similarity_calculator = CosineSimilarityCalculator(self.hyperparameter_space)
        self.main_output_dir = output_dir or "./results"
        self.population = []
        self.reference_set = []
        self.best_solutions = []
        self.iteration_history = []
        self.start_time = None
        self.current_iteration = 0
        
    def load_config(self, config_path: str):
        """Carga configuraci√≥n del algoritmo"""
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
            
        self.algo_config = self.config['algorithm']
        self.eval_config = self.config['evaluation']
        self.output_config = self.config['output']
        
    def load_from_checkpoint(self, checkpoint_path: str) -> bool:
        """Carga estado desde checkpoint y prepara para continuar"""
        try:
            with open(checkpoint_path, 'r') as f:
                checkpoint_data = json.load(f)
            
            self.current_iteration = checkpoint_data['iteration']
            self.reference_set = checkpoint_data['reference_set']
            self.best_solutions = checkpoint_data['best_solutions']
            self.iteration_history = checkpoint_data['iteration_history']
            
            print(f" Checkpoint cargado: iteraci√≥n {self.current_iteration}")
            return True
        except Exception as e:
            print(f" Error cargando checkpoint: {e}")
            return False
        
    def run_optimization(self, resume_from: str = None) -> Dict[str, Any]:
        """Ejecuta el algoritmo completo de Scatter Search - SIN MODO EMERGENCY"""
        print("Iniciando Scatter Search Optimization")
        print(f"Tiempo m√°ximo: {self.algo_config['max_time_hours']} horas")
        print(f"Poblaci√≥n: {self.algo_config['population_size']}")
        print(f"RefSet: {self.algo_config['ref_set_size']}")
        
        if resume_from:
            print(f"Intentando resumir desde: {resume_from}")
            if self.load_from_checkpoint(resume_from):
                return self._resume_optimization_loop()
            else:
                print("No se pudo cargar checkpoint, iniciando desde cero")
        
        print("Iniciando nueva optimizaci√≥n")
        self.start_time = time.time()
        
        # REMOVER try-except que activa emergency mode
        print("\nFase 1: Generando poblaci√≥n inicial...")
        self.population = self._generate_initial_population()
        
        print("\nFase 2: Construyendo Reference Set inicial...")
        self.reference_set = self._build_initial_reference_set()
        
        print("\nFase 3: Iteraciones principales...")
        self._main_optimization_loop()
        
        print("\nFase 4: Evaluaci√≥n final...")
        final_results = self._final_evaluation()
        
        return final_results
    
    def _log_archetype_distribution(self, population: List[Dict[str, Any]], 
                                    phase_name: str, detailed: bool = False):
        """Imprime distribuci√≥n de arquetipos en una poblaci√≥n"""
        
        archetype_counts = {}
        archetype_details = {}
        
        for i, solution in enumerate(population):
            archetype = self._classify_solution_archetype(solution)
            
            if archetype not in archetype_counts:
                archetype_counts[archetype] = 0
                archetype_details[archetype] = []
            
            archetype_counts[archetype] += 1
            
            if detailed:
                energy_sat = solution.get('energy_satisfaction_weight', 0)
                energy_cost = solution.get('energy_cost_weight', 0)
                penalty = solution.get('penalty_skipped_vehicle', 0)
                reward = solution.get('reward_assigned_vehicle', 0)
                
                archetype_details[archetype].append({
                    'index': i + 1,
                    'energy_sat': energy_sat,
                    'energy_cost': energy_cost,
                    'penalty': penalty,
                    'reward': reward
                })
        
        total = len(population)
        print(f"\nDISTRIBUCION DE ARQUETIPOS - {phase_name}")
        print("=" * 60)
        
        for archetype, count in sorted(archetype_counts.items()):
            percentage = (count / total) * 100
            print(f"   {archetype:20s}: {count:3d}/{total} ({percentage:5.1f}%)")
        
        if detailed and len(population) <= 50:
            print(f"\nDETALLES POR ARQUETIPO:")
            for archetype, details in archetype_details.items():
                print(f"\n   {archetype.upper()}:")
                for detail in details[:3]:
                    print(f"      {detail['index']:3d}/{total}: Sat={detail['energy_sat']:.2f}, "
                        f"Cost={detail['energy_cost']:.3f}, Pen={detail['penalty']:.0f}, "
                        f"Rew={detail['reward']:.0f}")
                if len(details) > 3:
                    print(f"      ... y {len(details)-3} mas")
        
        print("=" * 60)
        return archetype_counts
            
    def _generate_initial_population(self) -> List[Dict[str, Any]]:
        """Genera poblaci√≥n inicial con diversidad garantizada y logging"""
        population = []
        pop_size = self.algo_config['population_size']
        
        archetype_names = list(self.hyperparameter_space.archetypes.keys())
        individuals_per_archetype = max(2, pop_size // (len(archetype_names) * 3))
        
        print(f"\nGENERANDO POBLACION INICIAL:")
        print(f"   Total individuos: {pop_size}")
        print(f"   Arquetipos disponibles: {len(archetype_names)}")
        print(f"   Individuos por arquetipo: {individuals_per_archetype}")
        
        for archetype in archetype_names:
            print(f"   Generando {individuals_per_archetype} individuos de '{archetype}'...")
            for _ in range(individuals_per_archetype):
                solution = self.hyperparameter_space.generate_archetype_solution(archetype)
                population.append(solution)
                
        remaining = pop_size - len(population)
        if remaining > 0:
            print(f"   Generando {remaining} individuos aleatorios...")
            while len(population) < pop_size:
                solution = self.hyperparameter_space.generate_random_solution()
                population.append(solution)
        
        print(f"\nPOBLACION GENERADA COMPLETADA")
        actual_counts = self._log_archetype_distribution(population, "POBLACION INICIAL", detailed=True)
        
        for archetype in archetype_names:
            expected = individuals_per_archetype
            actual = actual_counts.get(archetype, 0)
            if actual < expected:
                print(f"WARNING: {archetype} genero solo {actual}/{expected} individuos")
        
        return population
        
    def _build_initial_reference_set(self) -> List[Dict[str, Any]]:
        """Construye el Reference Set inicial con logging"""
        print("Evaluando poblaci√≥n inicial...")
        
        fitness_scores = self._evaluate_population(self.population, level="fast")
        
        evaluated_population = list(zip(self.population, fitness_scores))
        evaluated_population.sort(key=lambda x: x[1], reverse=True)
        
        ref_set_size = self.algo_config['ref_set_size']
        elite_count = self.algo_config['elite_count']
        diverse_count = self.algo_config['diverse_count']
        
        reference_set = []
        
        for i in range(elite_count):
            reference_set.append(evaluated_population[i][0])
            
        remaining_solutions = [sol for sol, _ in evaluated_population[elite_count:]]
        diverse_solutions = self.similarity_calculator.find_most_diverse_configs(
            remaining_solutions, reference_set, diverse_count
        )
        reference_set.extend(diverse_solutions)
        
        print(f"RefSet construido: {len(reference_set)} soluciones")
        print(f"   - Elite: {elite_count}")
        print(f"   - Diverse: {len(diverse_solutions)}")
        
        self._log_archetype_distribution(reference_set, "REFERENCE SET INICIAL")
        
        self.best_solutions = [sol for sol, _ in evaluated_population[:5]]
        self._log_archetype_distribution(self.best_solutions, "TOP 5 MEJORES")
        
        return reference_set
    
    def _resume_optimization_loop(self):
        """Contin√∫a el loop principal desde donde se qued√≥"""
        print(f"üöÄ Resumiendo desde iteraci√≥n {self.current_iteration + 1}")
        
        max_iterations = self.algo_config['max_iterations']
        max_time = self.algo_config['max_time_hours'] * 3600
        
        if self.iteration_history:
            elapsed_time_from_history = self.iteration_history[-1]['time']
            self.start_time = time.time() - elapsed_time_from_history
            print(f"Tiempo ya transcurrido: {elapsed_time_from_history/3600:.2f}h")
        else:
            self.start_time = time.time()
            print("No hay historial de tiempo, reiniciando contador")
        
        try:
            start_iteration = self.current_iteration + 1
            print(f"RefSet actual: {len(self.reference_set)} soluciones")
            print(f"Mejores soluciones: {len(self.best_solutions)}")
            
            for iteration in range(start_iteration, max_iterations):
                self.current_iteration = iteration
                iteration_start = time.time()
                
                elapsed_time = time.time() - self.start_time
                if elapsed_time > max_time:
                    print(f"L√≠mite de tiempo alcanzado ({elapsed_time/3600:.1f}h)")
                    break
                    
                print(f"\nIteraci√≥n {iteration + 1}/{max_iterations} (RESUMIDA)")
                
                print(" Fase de combinaci√≥n...")
                new_solutions = self._combination_phase()
                
                print(" Fase de mejora...")
                improved_solutions = self._improvement_phase(new_solutions)
                
                print(" Actualizando RefSet...")
                self.reference_set = self._update_reference_set(improved_solutions)
                
                iteration_time = time.time() - iteration_start
                self._log_iteration_progress(iteration, iteration_time)
                
                checkpoint_frequency = self.output_config.get('save_frequency', 1)  
                if iteration % checkpoint_frequency == 0:
                    self._save_checkpoint(iteration)
                    print(f"Checkpoint autom√°tico guardado")
            
            print("\nFase 4: Evaluaci√≥n final...")
            final_results = self._final_evaluation()
            
            final_results['resumed_from_iteration'] = start_iteration - 1
            final_results['total_iterations_including_resume'] = len(self.iteration_history)
            
            return final_results
            
        except KeyboardInterrupt:
            print("\nOptimizaci√≥n resumida interrumpida por usuario")

        except Exception as e:
            print(f"\nError durante optimizaci√≥n resumida: {e}")

        
    def _main_optimization_loop(self):
        """Loop principal de optimizaci√≥n"""
        max_iterations = self.algo_config['max_iterations']
        max_time = self.algo_config['max_time_hours'] * 3600
        
        for iteration in range(max_iterations):
            self.current_iteration = iteration
            iteration_start = time.time()
            
            elapsed_time = time.time() - self.start_time
            if elapsed_time > max_time:
                print(f"L√≠mite de tiempo alcanzado ({elapsed_time/3600:.1f}h)")
                break
                
            print(f"\nIteraci√≥n {iteration + 1}/{max_iterations}")
            
            print("  Fase de combinaci√≥n...")
            new_solutions = self._combination_phase()
            
            print("  Fase de mejora...")
            improved_solutions = self._improvement_phase(new_solutions)
            
            print("  Actualizando RefSet...")
            self.reference_set = self._update_reference_set(improved_solutions)
            
            iteration_time = time.time() - iteration_start
            self._log_iteration_progress(iteration, iteration_time)
            
            if iteration % self.output_config.get('save_frequency', 2) == 0:
                self._save_checkpoint(iteration)
                
    def _combination_phase(self) -> List[Dict[str, Any]]:
        """Combina hiperpar√°metros del RefSet para crear nuevas soluciones con logging"""
        new_solutions = []
        
        print(f"    RefSet actual:")
        self._log_archetype_distribution(self.reference_set, "REFSET ANTES DE COMBINACION")
        
        for i in range(len(self.reference_set)):
            for j in range(i + 1, len(self.reference_set)):
                if random.random() < self.algo_config['combination_probability']:
                    child = self._combine_hyperparameters(
                        self.reference_set[i], self.reference_set[j]
                    )
                    new_solutions.append(child)
        
        if new_solutions:
            self._log_archetype_distribution(new_solutions, "SOLUCIONES COMBINADAS")
        
        filtered_solutions = self._filter_similar_solutions(new_solutions, self.current_iteration)
        
        if filtered_solutions:
            self._log_archetype_distribution(filtered_solutions, "SOLUCIONES FILTRADAS")
        
        print(f"    Generadas {len(new_solutions)} combinaciones, {len(filtered_solutions)} √∫nicas")
        return filtered_solutions
        
    def _combine_hyperparameters(self, parent1: Dict[str, Any], parent2: Dict[str, Any]) -> Dict[str, Any]:
        """Combina hiperpar√°metros de dos configuraciones padre"""
        child = {}
        alpha = random.uniform(0.3, 0.7)
        
        for param, config in self.hyperparameter_space.dqn_params.items():
            if config['type'] == 'float':
                child[param] = alpha * parent1[param] + (1 - alpha) * parent2[param]
                child[param] = np.clip(child[param], config['min'], config['max'])
            elif config['type'] == 'int_discrete':
                child[param] = random.choice([parent1[param], parent2[param]])
            elif config['type'] == 'bool':
                child[param] = random.choice([parent1[param], parent2[param]])
        
        for param, config in self.hyperparameter_space.reward_weights.items():
            child[param] = alpha * parent1[param] + (1 - alpha) * parent2[param]
            child[param] = np.clip(child[param], config['min'], config['max'])
        
        return child
        
    def _improvement_phase(self, solutions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Fase de mejora local"""
        improved_solutions = []
        
        for solution in solutions:
            if random.random() < self.algo_config['improvement_probability']:
                improved = self._local_improvement(solution)
                improved_solutions.append(improved)
            else:
                improved_solutions.append(solution)
                
        print(f"    Mejoradas {len(improved_solutions)} soluciones")
        return improved_solutions
        
    def _local_improvement(self, solution: Dict[str, Any]) -> Dict[str, Any]:
        """Aplica mejora local a una configuraci√≥n de hiperpar√°metros"""
        improved = solution.copy()
        
        all_params = list(solution.keys())
        param_to_improve = random.choice(all_params)
        
        param_config = (self.hyperparameter_space.dqn_params.get(param_to_improve) or 
                       self.hyperparameter_space.reward_weights.get(param_to_improve))
        
        if param_config:
            if param_config['type'] == 'float':
                param_range = param_config['max'] - param_config['min']
                std_dev = 0.05 * param_range
                perturbation = np.random.normal(0, std_dev)
                new_value = solution[param_to_improve] + perturbation
                improved[param_to_improve] = np.clip(new_value, param_config['min'], param_config['max'])
                
            elif param_config['type'] == 'int_discrete':
                values_list = param_config['values']
                current_index = values_list.index(solution[param_to_improve])
                possible_indices = []
                if current_index > 0:
                    possible_indices.append(current_index - 1)
                if current_index < len(values_list) - 1:
                    possible_indices.append(current_index + 1)
                if possible_indices:
                    new_index = random.choice(possible_indices)
                    improved[param_to_improve] = values_list[new_index]
                    
            elif param_config['type'] == 'bool':
                improved[param_to_improve] = not solution[param_to_improve]
        
        return improved
        
    def _get_adaptive_similarity_threshold(self, iteration: int) -> float:
        """Threshold basado en mejora del fitness Y progreso temporal"""
        
        max_iterations = self.algo_config['max_iterations']
        progress = iteration / max_iterations
        temporal_threshold = 0.95 - (0.20 * progress)  
        
        if len(self.iteration_history) >= 3:
            recent_fitness = [h['best_fitness'] for h in self.iteration_history[-3:]]
            improvement = (recent_fitness[-1] - recent_fitness[0]) / abs(recent_fitness[0] + 1e-6)
            
            if improvement > 0.05: 
                fitness_threshold = 0.80  # Explotar
            elif improvement > 0.01: 
                fitness_threshold = 0.85  # Balance
            else: 
                fitness_threshold = 0.95  # Explorar
            
            threshold = 0.6 * fitness_threshold + 0.4 * temporal_threshold
        else:
            threshold = temporal_threshold
        
        return np.clip(threshold, 0.75, 0.95)

    def _filter_similar_solutions(self, solutions: List[Dict[str, Any]], iteration: int = 0) -> List[Dict[str, Any]]:
        """Filtra configuraciones con threshold adaptativo"""
        if not solutions:
            return solutions
        
        threshold = self._get_adaptive_similarity_threshold(iteration)
        
        filtered = [solutions[0]]
        
        for solution in solutions[1:]:
            is_unique = True
            for existing in filtered:
                if self.similarity_calculator.is_too_similar(solution, existing, threshold):
                    is_unique = False
                    break
            if is_unique:
                filtered.append(solution)
        
        print(f"    Threshold similitud: {threshold:.2f}, Filtradas: {len(solutions)} -> {len(filtered)}")
        return filtered
        
    def _update_reference_set(self, new_solutions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Actualiza el Reference Set con logging de arquetipos"""
        all_candidates = self.reference_set + new_solutions
        
        print(f"    Evaluando {len(all_candidates)} candidatos para RefSet...")
        self._log_archetype_distribution(all_candidates, "CANDIDATOS PARA REFSET")
        
        fitness_scores = self._evaluate_population(all_candidates, level="medium")
        
        new_ref_set = self._select_diverse_reference_set(
            all_candidates, fitness_scores, self.algo_config['ref_set_size']
        )
        
        self._log_archetype_distribution(new_ref_set, "NUEVO REFERENCE SET")
        
        evaluated_candidates = list(zip(all_candidates, fitness_scores))
        evaluated_candidates.sort(key=lambda x: x[1], reverse=True)
        self.best_solutions = [sol for sol, _ in evaluated_candidates[:5]]
        
        self._log_archetype_distribution(self.best_solutions, "NUEVOS TOP 5 MEJORES")
        
        print(f"    RefSet actualizado")
        return new_ref_set
        
    def _select_diverse_reference_set(self, population: List[Dict[str, Any]], 
                                     fitness_scores: List[float], ref_set_size: int) -> List[Dict[str, Any]]:
        """Selecciona RefSet balanceando calidad y diversidad usando similitud coseno"""
        
        sorted_configs = sorted(zip(population, fitness_scores), key=lambda x: x[1], reverse=True)
        
        elite_count = max(1, int(ref_set_size * 0.6))
        elite_configs = [config for config, _ in sorted_configs[:elite_count]]
        
        remaining_configs = [config for config, _ in sorted_configs[elite_count:]]
        diverse_count = ref_set_size - elite_count
        
        if diverse_count > 0 and remaining_configs:
            diverse_configs = self.similarity_calculator.find_most_diverse_configs(
                remaining_configs, elite_configs, diverse_count
            )
            return elite_configs + diverse_configs
        else:
            return elite_configs[:ref_set_size]
    
    def _evaluate_population(self, population: List[Dict[str, Any]], level: str = "fast") -> List[float]:
        """Eval√∫a una poblaci√≥n de configuraciones de hiperpar√°metros"""
        fitness_scores = []
        
        if level == "fast":
            systems_to_use = [self.systems_data[i] for i in self.eval_config['fast']['systems']]
            episodes = self.eval_config['fast']['episodes']
        elif level == "medium":
            systems_to_use = [self.systems_data[i] for i in self.eval_config['medium']['systems']]
            episodes = self.eval_config['medium']['episodes']
        else:  
            systems_to_use = list(self.systems_data.values())
            episodes = self.eval_config['full']['episodes']
        
        print(f"    Evaluando {len(population)} configuraciones en {len(systems_to_use)} sistemas ({episodes} episodios)")
        
        total_configs = len(population)
        elapsed_time = time.time() - self.start_time if self.start_time else 0
        max_time = self.algo_config['max_time_hours'] * 3600
        
        # Determinar fase actual
        if not hasattr(self, 'current_iteration') or self.current_iteration == 0:
            if not hasattr(self, '_initial_phase_done'):
                phase = "INICIAL"
            else:
                phase = f"ITER_{self.current_iteration + 1}"
        elif level == "full":
            phase = "FINAL"
        else:
            phase = f"ITER_{self.current_iteration + 1}"
        
        for i, config in enumerate(population):
            print(f"   ############## Fase {phase} - Poblacion {i+1}/{total_configs} - Tiempo {elapsed_time/3600:.2f}h/{max_time/3600:.0f}h")
            
            dqn_params = self._extract_dqn_params(config)
            reward_weights = self._extract_reward_weights(config)
            
            system_fitness = []
            for system_config in systems_to_use:
                try:
                    fitness = self._evaluate_single_config(dqn_params, reward_weights, system_config, episodes)
                    system_fitness.append(fitness)
                except Exception as e:
                    if "missing" in str(e) or "Error" in str(e):
                        print(f"        Error en configuracion {i+1}: {str(e)[:50]}...")
                    system_fitness.append(-1000.0)
            
            avg_fitness = np.mean(system_fitness)
            fitness_scores.append(avg_fitness)
            
            # Actualizar tiempo para siguiente iteraci√≥n
            elapsed_time = time.time() - self.start_time if self.start_time else 0
        
        best_fitness = max(fitness_scores)
        worst_fitness = min(fitness_scores)
        avg_fitness_all = np.mean(fitness_scores)
        
        print(f"     Evaluacion completada:")
        print(f"       Mejor fitness: {best_fitness:.2f}")
        print(f"       Peor fitness: {worst_fitness:.2f}")
        print(f"       Promedio: {avg_fitness_all:.2f}")
        
        return fitness_scores
    
    def _evaluate_single_config(self, dqn_params: Dict, reward_weights: Dict, 
                           system_config: Dict, episodes: int) -> float:
        """Eval√∫a una configuraci√≥n en un sistema espec√≠fico"""
        
        env = EVChargingEnv(system_config)
        env.update_reward_weights(reward_weights)
        
        state_size = 40 
        action_size = 60  
        
        agent = EnhancedDQNAgentPyTorch(state_size, action_size, **dqn_params)
        
        results = train_dqn_agent(agent, env, episodes,verbose=False)
        
        if len(results) >= 10:
            recent_rewards = [ep['reward'] for ep in results[-10:]]
        else:
            recent_rewards = [ep['reward'] for ep in results]
        
        return np.mean(recent_rewards)
    
    def _extract_dqn_params(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Extrae hiperpar√°metros DQN de una configuraci√≥n"""
        dqn_param_names = [
            'learning_rate', 'gamma', 'epsilon_start', 'epsilon_min',
            'epsilon_decay', 'batch_size', 'target_update_freq', 
            'memory_size', 'dueling_network'
        ]
        
        dqn_params = {}
        for param in dqn_param_names:
            if param in config:
                if param == 'epsilon_start':
                    dqn_params['epsilon'] = config[param]
                else:
                    dqn_params[param] = config[param]
        
        return dqn_params
    
    def _extract_reward_weights(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Extrae pesos de recompensa de una configuraci√≥n"""
        weight_param_names = [
            'energy_satisfaction_weight', 'energy_cost_weight',
            'penalty_skipped_vehicle', 'reward_assigned_vehicle'
        ]
        
        return {param: config[param] for param in weight_param_names if param in config}
    
    def _train_and_save_best_models(self, candidates, fitness_scores):
        """Entrena y guarda solo los mejores modelos - CORREGIDO"""
        print("Entrenando y guardando modelos √≥ptimos...")
        
        models_dir = os.path.join(self.main_output_dir, "trained_models")
        os.makedirs(models_dir, exist_ok=True)
        
        for i, (config, fitness) in enumerate(zip(candidates, fitness_scores)):
            if fitness > -500:  
                print(f"Entrenando modelo √≥ptimo {i+1}/5...")
                
                # CORREGIR: usar m√©todo que S√ç existe
                dqn_params = self._extract_dqn_params(config)
                reward_weights = self._extract_reward_weights(config)
                
                # Usar el primer sistema para entrenamiento final
                first_system = list(self.systems_data.values())[0]
                env = EVChargingEnv(first_system)
                env.update_reward_weights(reward_weights)
                
                # Crear y entrenar agente
                state_size = 40 
                action_size = 60  
                agent = EnhancedDQNAgentPyTorch(state_size, action_size, **dqn_params)
                
                # Entrenar con m√°s episodios para modelo final
                train_dqn_agent(agent, env, num_episodes=100, verbose=False)
                
                archetype = self._classify_solution_archetype(config)
                model_path = f"{models_dir}/{archetype}_rank_{i+1}.pt"
                agent.save(model_path)
                
                print(f"Modelo guardado: {model_path}")

    def _final_evaluation(self) -> Dict[str, Any]:
        """Evaluaci√≥n final completa de los mejores candidatos - CORREGIDO"""
        print("Evaluaci√≥n final completa...")
        
        top_candidates = self.best_solutions[:5]
        final_fitness = self._evaluate_population(top_candidates, level="full")
        
        # CORREGIR: Solo llamar si hay candidatos v√°lidos
        if top_candidates and any(f > -500 for f in final_fitness):
            try:
                self._train_and_save_best_models(top_candidates, final_fitness)
            except Exception as e:
                print(f"Warning: No se pudieron entrenar modelos finales: {e}")

        final_results = []
        for i, (solution, fitness) in enumerate(zip(top_candidates, final_fitness)):
            archetype = self._classify_solution_archetype(solution)
            
            result = {
                'rank': i + 1,
                'fitness': fitness,
                'archetype': archetype,
                'hyperparameters': solution,
                'configuration_name': f"optimized_{archetype}_{i+1}"  # CORREGIR typo
            }
            final_results.append(result)
            
        final_results.sort(key=lambda x: x['fitness'], reverse=True)
        
        return {
            'best_solutions': final_results,
            'optimization_summary': self._create_optimization_summary(),
            'execution_time': time.time() - self.start_time
        }
    
    def _classify_solution_archetype(self, solution: Dict[str, Any]) -> str:
        """Clasifica una soluci√≥n seg√∫n su arquetipo de comportamiento - VERSI√ìN MEJORADA"""
        
        energy_satisfaction = solution.get('energy_satisfaction_weight', 1.0)
        energy_cost = solution.get('energy_cost_weight', 0.1)
        penalty_skipped = solution.get('penalty_skipped_vehicle', 50.0)
        reward_assigned = solution.get('reward_assigned_vehicle', 20.0)
        
        # Clasificaci√≥n m√°s granular para obtener diversidad
        cost_ratio = energy_cost / max(energy_satisfaction, 0.1)
        
        # 1. Cost Minimizer: Prioriza costos
        if energy_cost > 1.0 or cost_ratio > 0.4:
            return "cost_minimizer"
        
        # 2. Urgency Focused: Penalizaci√≥n muy alta por saltar
        elif penalty_skipped > 200.0:
            return "urgency_focused"
        
        # 3. Efficiency Focused: Recompensa alta por asignar
        elif reward_assigned > 100.0:
            return "efficiency_focused"
        
        # 4. Satisfaction Maximizer: SOLO si satisfaction es muy alto Y cost bajo
        elif energy_satisfaction > 4.0 and energy_cost < 0.5:
            return "satisfaction_maximizer"
        
        # 5. Balanced Optimizer: El resto (ser√° la mayor√≠a ahora)
        else:
            return "balanced_optimizer"
        
    def _log_iteration_progress(self, iteration: int, iteration_time: float):
        """Log del progreso de la iteraci√≥n"""
        elapsed_total = time.time() - self.start_time
        
        if self.best_solutions:
            best_config = self.best_solutions[0]
            dqn_params = self._extract_dqn_params(best_config)
            reward_weights = self._extract_reward_weights(best_config)
            
            first_system = list(self.systems_data.values())[0]
            current_best_fitness = self._evaluate_single_config(
                dqn_params, reward_weights, first_system, 10
            )
        else:
            current_best_fitness = 0
            
        print(f"      Tiempo iteraci√≥n: {iteration_time:.1f}s")
        print(f"      Tiempo total: {elapsed_total/3600:.2f}h")
        print(f"      Mejor fitness actual: {current_best_fitness:.2f}")
        
        diversity_metrics = self.similarity_calculator.calculate_diversity_metrics(self.reference_set)
        print(f"      Diversidad RefSet: {diversity_metrics['diversity_score']:.3f}")
        
        self.iteration_history.append({
            'iteration': iteration + 1,
            'time': elapsed_total,
            'best_fitness': current_best_fitness,
            'iteration_duration': iteration_time,
            'diversity_score': diversity_metrics['diversity_score']
        })
    
    def _create_optimization_summary(self) -> Dict[str, Any]:
        """Crea resumen de la optimizaci√≥n"""
        try:
            if not self.iteration_history:
                return {}
            
            return {
                'total_iterations': len(self.iteration_history),
                'total_time_hours': (time.time() - self.start_time) / 3600,
                'final_best_fitness': self.iteration_history[-1]['best_fitness'],
                'initial_best_fitness': self.iteration_history[0]['best_fitness'],
                'improvement': self.iteration_history[-1]['best_fitness'] - self.iteration_history[0]['best_fitness'],
                'final_diversity': self.iteration_history[-1]['diversity_score'],
                'iteration_history': self.iteration_history
            }
        except Exception as e:
            print(f"Error on create_optimization_summary as  : {e}")

    def _save_checkpoint(self, iteration: int):
        """Guarda checkpoint del algoritmo"""
        try:
            checkpoint_dir = os.path.join(self.main_output_dir, "checkpoints")
            os.makedirs(checkpoint_dir, exist_ok=True)

            checkpoint_data = {
                'iteration': iteration,
                'reference_set': self.reference_set,
                'best_solutions': self.best_solutions,
                'iteration_history': self.iteration_history,
                'config': self.config
            }
            
            checkpoint_path = os.path.join(checkpoint_dir, f"scatter_checkpoint_iter_{iteration}.json")        
            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
            
            with open(checkpoint_path, 'w') as f:
                json.dump(checkpoint_data, f, indent=2, default=str)
            
            print(f"      Checkpoint guardado: {checkpoint_path}")
        except Exception as e:
            print(f"Error on savecheckpoint: {e}")

    def _emergency_results(self) -> Dict[str, Any]:
        """Retorna resultados de emergencia en caso de error"""
        
        emergency_solutions = []
        
        if hasattr(self, 'best_solutions') and self.best_solutions:
            print("‚ö° Evaluaci√≥n r√°pida de emergencia para obtener fitness real...")
            
            # Evaluar con nivel "fast" para obtener fitness real r√°pidamente
            try:
                emergency_fitness = self._evaluate_population(self.best_solutions, level="fast")
                
                for i, (solution, fitness) in enumerate(zip(self.best_solutions, emergency_fitness)):
                    emergency_solutions.append({
                        'rank': i + 1,
                        'fitness': fitness,  # ‚Üê FITNESS REAL evaluado r√°pidamente
                        'archetype': self._classify_solution_archetype(solution),
                        'hyperparameters': solution,
                        'configuration_name': f"emergency_{i+1}"
                    })
            except Exception as e:
                print(f"‚ö†Ô∏è No se pudo evaluar fitness en emergencia: {e}")
                # Solo si falla completamente, entonces no incluir fitness
                for i, solution in enumerate(self.best_solutions):
                    emergency_solutions.append({
                        'rank': i + 1,
                        # 'fitness': NO INCLUIR LA CLAVE SI NO LA TENEMOS
                        'archetype': self._classify_solution_archetype(solution),
                        'hyperparameters': solution,
                        'configuration_name': f"emergency_{i+1}",
                        'note': 'fitness_not_evaluated'
                    })
        
        return {
            'best_solutions': emergency_solutions,
            'optimization_summary': self._create_optimization_summary(),
            'execution_time': time.time() - self.start_time if self.start_time else 0,
            'status': 'interrupted'
        }