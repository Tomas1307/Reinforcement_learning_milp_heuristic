ev_charging_optimization/
â”œâ”€â”€ src/                                   # CÃ³digo fuente principal de la aplicaciÃ³n
â”‚   â”œâ”€â”€ common/                            # Utilidades compartidas por mÃºltiples mÃ³dulos
â”‚   â”‚   â”œâ”€â”€ config.py                      # Para cargar y gestionar configuraciones (hiperparÃ¡metros, sistemas)
â”‚   â”‚   â”œâ”€â”€ logger.py                      # AbstracciÃ³n para el registro de eventos (basado en train_logger.py)
â”‚   â”‚   â””â”€â”€ utils.py                       # Funciones utilitarias generales (e.g., carga de datos, procesamiento JSON)
â”‚   â”‚   â””â”€â”€ metrics.py                     # CÃ¡lculo de mÃ©tricas de soluciÃ³n (para fitness y diversidad)
â”‚   â”‚   â””â”€â”€ visualize.py                   # Funciones de visualizaciÃ³n generales (basado en visualize.py)
â”‚   â”‚
â”‚   â”œâ”€â”€ dqn_agent/                         # MÃ³dulo para el agente DQN
â”‚   â”‚   â”œâ”€â”€ agent.py                       # Tu actual DQN_agent.py (renombrado a 'agent.py')
â”‚   â”‚   â”œâ”€â”€ environment.py                 # Tu actual EV_env.py (renombrado a 'environment.py')
â”‚   â”‚   â”œâ”€â”€ training.py                    # LÃ³gica para entrenar un solo agente DQN (parte de tu actual main.py)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ milp_optimizer/                    # MÃ³dulo para el optimizador MILP
â”‚   â”‚   â”œâ”€â”€ optimizer.py                   # Tu actual MILP_optimizer.py (renombrado a 'optimizer.py')
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ scatter_search/                    # MÃ³dulo para la metaheurÃ­stica Scatter Search
â”‚       â”œâ”€â”€ core.py                        # ImplementaciÃ³n principal del algoritmo Scatter Search
â”‚       â”œâ”€â”€ operators.py                   # Operadores de combinaciÃ³n y diversidad para DQN
â”‚       â”œâ”€â”€ evaluator.py                   # LÃ³gica para evaluar un 'individuo' DQN (entrenar y obtener soluciÃ³n final)
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ config/                                # Archivos de configuraciÃ³n de alto nivel
â”‚   â”œâ”€â”€ system_configs/                    # Configuraciones de los sistemas de carga (tus test_system_X.json)
â”‚   â”‚   â”œâ”€â”€ test_system_1.json
â”‚   â”‚   â”œâ”€â”€ test_system_2.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ scatter_search_params.yaml         # ParÃ¡metros para Scatter Search (tamaÃ±o de poblaciÃ³n, criterios, etc.)
â”‚   â””â”€â”€ dqn_hyperparameters.yaml           # Rangos o valores base de hiperparÃ¡metros para DQNâ”‚
â”œâ”€â”€ results/                               # AquÃ­ se guardarÃ¡n todos los resultados de los experimentos hÃ­bridos
â”‚   â”œâ”€â”€ scatter_search_runs/               # Resultados de cada ejecuciÃ³n del Scatter Search
â”‚   â”‚   â”œâ”€â”€ run_YYYYMMDD_HHMMSS/
â”‚   â”‚   â”‚   â”œâ”€â”€ population_history.json    # Estado de la poblaciÃ³n en cada iteraciÃ³n
â”‚   â”‚   â”‚   â”œâ”€â”€ best_dqn_configs/          # Configuraciones de los mejores DQN encontrados
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dqn_config_1.yaml
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â”œâ”€â”€ final_reference_set/       # Los DQN finales del Reference Set
â”‚   â”‚   â”‚   â””â”€â”€ plots/                     # GrÃ¡ficas de convergencia del Scatter Search
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ dqn_evaluations/                   # Resultados intermedios de los DQN evaluados por Scatter Search
â”‚   â”‚   â”œâ”€â”€ dqn_id_X_results.json
â”‚   â”‚   â”œâ”€â”€ dqn_id_Y_results.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ combined_analysis/                 # AnÃ¡lisis y visualizaciones de los resultados combinados
â”‚
â”œâ”€â”€ scripts/                               # Scripts ejecutables de alto nivel
â”‚   â”œâ”€â”€ run_scatter_dqn_optimization.py    # El script principal para ejecutar todo el proceso hÃ­brido
â”‚   â”œâ”€â”€ visualize_results.py               # Script para generar visualizaciones a partir de 'results/'
â”‚   â””â”€â”€ create_initial_population.py       # Script para generar las 3 primeras configuraciones de DQN
â”‚
â”‚
â”œâ”€â”€ tests/                                 # Pruebas unitarias y de integraciÃ³n
â”‚   â”œâ”€â”€ test_dqn_agent.py
â”‚   â”œâ”€â”€ test_milp_optimizer.py
â”‚   â””â”€â”€ test_scatter_search.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                              # Actualizar con la nueva estructura y el nuevo enfoque
â”œâ”€â”€ requirements.txt                       # Listar todas las dependencias4



ğŸ¯ RESUMEN COMPLETO: Scatter Search para OptimizaciÃ³n de HiperparÃ¡metros DQN
ğŸ“‹ OBJETIVO PRINCIPAL
Implementar Scatter Search para encontrar la combinaciÃ³n Ã³ptima de hiperparÃ¡metros del DQN + pesos de funciÃ³n de recompensa que maximice el rendimiento en sistemas de carga de vehÃ­culos elÃ©ctricos, ejecutÃ¡ndose durante 48 horas en hardware potente.

ğŸ”§ CONFIGURACIÃ“N DEL ALGORITMO
ParÃ¡metros Scatter Search:

PoblaciÃ³n inicial: 150 individuos
Reference Set (RefSet): 10 soluciones

DivisiÃ³n: b1 (Ã©lite) + b2 (diversas)


Tiempo total: 48 horas
ParalelizaciÃ³n: MÃºltiples workers
Iteraciones estimadas: 2-6 (dependiendo de evaluaciÃ³n)

Estrategia de EvaluaciÃ³n Multi-Nivel:
pythonevaluation_levels = {
    "fast": {
        "episodes": 30,
        "systems": [1, 5],  # 2 sistemas representativos
        "time_per_eval": "~15 min",
        "usage": "PoblaciÃ³n inicial + exploraciÃ³n"
    },
    "medium": {
        "episodes": 100, 
        "systems": [1, 3, 5, 7],  # 4 sistemas
        "time_per_eval": "~45 min",
        "usage": "Refinamiento RefSet"
    },
    "full": {
        "episodes": 200,
        "systems": "todos",  # Sistemas completos
        "time_per_eval": "~2 horas", 
        "usage": "ValidaciÃ³n candidatos finales"
    }
}

ğŸ² ESPACIO DE BÃšSQUEDA (13 PARÃMETROS)
A. HiperparÃ¡metros DQN (9 parÃ¡metros):
ParÃ¡metros Continuos (5):
yamldqn_hyperparameters:
  learning_rate: 
    min: 0.0001
    max: 0.005
    type: float
    rationale: "Conservador para estabilidad"
    
  gamma: 
    min: 0.95
    max: 0.999
    type: float
    rationale: "Horizonte largo para planificaciÃ³n carga"
    
  epsilon_start: 
    min: 0.8
    max: 1.0
    type: float
    rationale: "ExploraciÃ³n inicial fuerte"
    
  epsilon_min: 
    min: 0.01
    max: 0.05
    type: float
    rationale: "Convergencia final estable"
    
  epsilon_decay: 
    min: 0.995
    max: 0.999
    type: float
    rationale: "Decaimiento lento para mejor exploraciÃ³n"
ParÃ¡metros Discretos (4):
yaml  batch_size: 
    values: [32, 64, 128, 256]
    type: int_discrete
    rationale: "TamaÃ±os estÃ¡ndar, eliminÃ© 16 (muy pequeÃ±o)"
    
  target_update_freq: 
    values: [10, 20, 30, 50, 100]
    type: int_discrete
    rationale: "Frecuencia actualizaciÃ³n target network"
    
  memory_size: 
    values: [10000, 20000, 50000]
    type: int_discrete
    rationale: "AumentÃ© valores para mejor experience replay"
    
  dueling_network: 
    values: [True, False]
    type: bool
    rationale: "Arquitectura Dueling vs estÃ¡ndar"
B. Pesos de FunciÃ³n de Recompensa (4 parÃ¡metros):
yamlreward_weights:
  energy_satisfaction_weight: 
    min: 0.3
    max: 3.0
    type: float
    current_value: 1.0
    rationale: "Permite desde econÃ³micos hasta ultra-satisfaction"
    
  energy_cost_weight: 
    min: 0.01
    max: 1.0
    type: float
    current_value: 0.1
    rationale: "Desde ignorar costos hasta ultra-cost-focused"
    
  penalty_skipped_vehicle: 
    min: 10.0
    max: 200.0
    type: float
    current_value: 50.0
    rationale: "Desde permisivo hasta estricto absoluto"
    
  reward_assigned_vehicle: 
    min: 1.0
    max: 100.0
    type: float
    current_value: 20.0
    rationale: "Incentivos desde mÃ­nimos hasta mÃ¡ximos"

ğŸ—ï¸ ARQUITECTURA DE IMPLEMENTACIÃ“N
Estructura de Archivos Nuevos:
src/optimization/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ scatter_search.py           # Algoritmo principal
â”œâ”€â”€ hyperparameter_space.py     # DefiniciÃ³n del espacio de bÃºsqueda
â”œâ”€â”€ evaluation_strategies.py    # Estrategias multi-nivel
â”œâ”€â”€ diversity_metrics.py        # MÃ©tricas de diversidad
â””â”€â”€ solution_generation.py      # GeneraciÃ³n y combinaciÃ³n

configs/
â”œâ”€â”€ scatter_search_params.yaml  # ConfiguraciÃ³n algoritmo
â””â”€â”€ hyperparameter_ranges.yaml  # DefiniciÃ³n de rangos
Modificaciones a Archivos Existentes:
main.py:
python# Agregar nuevo modo
if args.mode == "optimize_hyperparameters":
    from src.optimization.scatter_search import ScatterSearchOptimizer
    optimizer = ScatterSearchOptimizer(config_path=args.config)
    best_solutions = optimizer.run_optimization()
environment.py:
pythonclass EVChargingEnv:
    def update_reward_weights(self, weights_dict):
        """Actualizar pesos dinÃ¡micamente"""
        self.ENERGY_SATISFACTION_WEIGHT = weights_dict["energy_satisfaction_weight"]
        self.ENERGY_COST_WEIGHT = weights_dict["energy_cost_weight"] 
        self.PENALTY_FOR_SKIPPED_VEHICLE = weights_dict["penalty_skipped_vehicle"]
        self.REWARD_FOR_ASSIGNED_VEHICLE = weights_dict["reward_assigned_vehicle"]
training.py:
pythondef train_dqn_agent_programmatic(hyperparameters, system_config, episodes=100):
    """VersiÃ³n callable para Scatter Search"""
    # Extraer hiperparÃ¡metros DQN
    dqn_params = extract_dqn_params(hyperparameters)
    reward_weights = extract_reward_weights(hyperparameters)
    
    # Crear agente y environment
    agent = EnhancedDQNAgent(**dqn_params)
    env = EVChargingEnv(system_config)
    env.update_reward_weights(reward_weights)
    
    # Entrenar
    results = train_dqn_agent(agent, env, episodes)
    
    return np.mean([ep['reward'] for ep in results[-10:]])  # FITNESS

ğŸ¯ DEFINICIÃ“N DE SOLUCIÃ“N
Una SoluciÃ³n = Conjunto Completo de HiperparÃ¡metros:
pythonexample_solution = {
    # DQN Hyperparameters  
    "learning_rate": 0.0025,
    "gamma": 0.97,
    "epsilon_start": 0.9,
    "epsilon_min": 0.02,
    "epsilon_decay": 0.997,
    "batch_size": 64,
    "target_update_freq": 30,
    "memory_size": 20000,
    "dueling_network": True,
    
    # Reward Weights
    "energy_satisfaction_weight": 1.5,
    "energy_cost_weight": 0.3,
    "penalty_skipped_vehicle": 75.0,
    "reward_assigned_vehicle": 40.0,
    
    # Metadata
    "fitness": 1250.5,  # Promedio de recompensas
    "archetype": "balanced_optimizer"
}

ğŸ”„ ALGORITMO SCATTER SEARCH DETALLADO
Paso 1: GeneraciÃ³n de PoblaciÃ³n Inicial
pythondef generate_initial_population(size=150):
    population = []
    
    # Asegurar diversidad de arquetipos
    archetypes = {
        "cost_minimizer": {"energy_cost_weight": [0.7, 1.0]},
        "satisfaction_maximizer": {"energy_satisfaction_weight": [2.0, 3.0]},
        "balanced_optimizer": {"energy_satisfaction_weight": [1.0, 1.5], 
                              "energy_cost_weight": [0.2, 0.5]},
        "urgency_focused": {"penalty_skipped_vehicle": [150, 200]},
        "efficiency_focused": {"reward_assigned_vehicle": [70, 100]}
    }
    
    # MÃ­nimo 10 individuos por arquetipo
    for archetype, constraints in archetypes.items():
        for _ in range(10):
            individual = generate_individual_with_constraints(constraints)
            population.append(individual)
    
    # Llenar resto con diversidad aleatoria
    while len(population) < size:
        individual = generate_random_individual()
        population.append(individual)
    
    return population
Paso 2: ConstrucciÃ³n RefSet
pythondef build_reference_set(population, ref_size=10):
    # Evaluar toda la poblaciÃ³n (nivel "fast")
    evaluated_pop = [(ind, evaluate_solution(ind, "fast")) for ind in population]
    
    # Ordenar por fitness
    evaluated_pop.sort(key=lambda x: x[1], reverse=True)
    
    # SelecciÃ³n balanceada calidad-diversidad
    refset = []
    
    # b1 = 6 mejores (Ã©lite)
    for i in range(6):
        refset.append(evaluated_pop[i][0])
    
    # b2 = 4 diversos (no solo por fitness)
    remaining = [ind for ind, _ in evaluated_pop[6:]]
    diverse_candidates = select_diverse_solutions(remaining, refset, 4)
    refset.extend(diverse_candidates)
    
    return refset
Paso 3: CombinaciÃ³n de Soluciones
pythondef combine_solutions(parent1, parent2):
    child = {}
    
    for param in hyperparameter_space:
        if param_type[param] == "float":
            # InterpolaciÃ³n con factor aleatorio
            alpha = random.uniform(0.2, 0.8)
            child[param] = alpha * parent1[param] + (1-alpha) * parent2[param]
            # Clip a rango vÃ¡lido
            child[param] = np.clip(child[param], min_val[param], max_val[param])
            
        elif param_type[param] == "int_discrete":
            # SelecciÃ³n aleatoria de uno de los padres
            child[param] = random.choice([parent1[param], parent2[param]])
            
        elif param_type[param] == "bool":
            # VotaciÃ³n o aleatorio
            child[param] = random.choice([parent1[param], parent2[param]])
    
    return child
Paso 4: Mejora Local
pythondef local_improvement(solution):
    improved = solution.copy()
    
    for param in hyperparameter_space:
        if param_type[param] == "float":
            # PerturbaciÃ³n pequeÃ±a (Â±5% del rango)
            range_size = max_val[param] - min_val[param] 
            delta = random.uniform(-0.05 * range_size, 0.05 * range_size)
            new_val = np.clip(solution[param] + delta, min_val[param], max_val[param])
            
            # Evaluar si mejora
            test_solution = improved.copy()
            test_solution[param] = new_val
            if evaluate_solution(test_solution, "fast") > evaluate_solution(improved, "fast"):
                improved[param] = new_val
                
        elif param_type[param] == "int_discrete":
            # Probar valores adyacentes en la lista
            current_idx = discrete_values[param].index(solution[param])
            for new_idx in [current_idx-1, current_idx+1]:
                if 0 <= new_idx < len(discrete_values[param]):
                    test_solution = improved.copy()
                    test_solution[param] = discrete_values[param][new_idx]
                    if evaluate_solution(test_solution, "fast") > evaluate_solution(improved, "fast"):
                        improved[param] = discrete_values[param][new_idx]
                        break
    
    return improved
Paso 5: ActualizaciÃ³n RefSet
pythondef update_reference_set(refset, new_solutions):
    # Combinar candidatos actuales + nuevos
    all_candidates = refset + new_solutions
    
    # Evaluar todos con nivel "medium"
    evaluated = [(sol, evaluate_solution(sol, "medium")) for sol in all_candidates]
    
    # Seleccionar nuevo RefSet balanceando calidad y diversidad
    new_refset = select_best_diverse_subset(evaluated, ref_size=10)
    
    return new_refset

â±ï¸ TIMELINE DETALLADO (48 HORAS)
Fase 1: InicializaciÃ³n y ExploraciÃ³n (Horas 0-12)

Horas 0-4: Generar poblaciÃ³n inicial (150 individuos)
Horas 4-8: Evaluar poblaciÃ³n completa (nivel "fast")
Horas 8-10: Construir RefSet inicial
Horas 10-12: Primera iteraciÃ³n de combinaciÃ³n y mejora

Fase 2: Refinamiento Intensivo (Horas 12-36)

Iteraciones 2-4: CombinaciÃ³n sistemÃ¡tica del RefSet
EvaluaciÃ³n "medium": Para candidatos prometedores
BÃºsqueda local agresiva: Mejora continua
ActualizaciÃ³n RefSet: Cada 6-8 horas

Fase 3: ValidaciÃ³n Final (Horas 36-48)

EvaluaciÃ³n "full": Top 5 candidatos del RefSet
ValidaciÃ³n cruzada: Todos los sistemas disponibles
AnÃ¡lisis comparativo: CaracterizaciÃ³n de arquetipos
SelecciÃ³n final: MÃºltiples agentes especializados


ğŸ† RESULTADOS ESPERADOS
MÃºltiples Agentes Especializados:
1. EconomyBot (Cost Minimizer)
pythonexpected_config = {
    "energy_satisfaction_weight": 0.3,
    "energy_cost_weight": 0.8,
    "penalty_skipped_vehicle": 15.0,
    "behavior": "Carga mÃ­nima necesaria en horarios baratos",
    "use_case": "Sistemas con presiÃ³n econÃ³mica"
}
2. PremiumBot (Satisfaction Maximizer)
pythonexpected_config = {
    "energy_satisfaction_weight": 2.5,
    "energy_cost_weight": 0.05,
    "reward_assigned_vehicle": 80.0,
    "behavior": "SatisfacciÃ³n completa, costo secundario",
    "use_case": "Sistemas premium o crÃ­ticos"
}
3. CriticalBot (Urgency Focused)
pythonexpected_config = {
    "penalty_skipped_vehicle": 180.0,
    "reward_assigned_vehicle": 90.0,
    "energy_satisfaction_weight": 2.0,
    "behavior": "Nunca deja vehÃ­culos sin atender",
    "use_case": "Hospitales, aeropuertos, servicios crÃ­ticos"
}
4. BalancedBot (Optimized General)
pythonexpected_config = {
    "energy_satisfaction_weight": 1.4,
    "energy_cost_weight": 0.35,
    "penalty_skipped_vehicle": 85.0,
    "reward_assigned_vehicle": 45.0,
    "behavior": "Ã“ptimo general mejorado",
    "use_case": "Sistemas comerciales estÃ¡ndar"
}
5. EfficiencyBot (Assignment Maximizer)
pythonexpected_config = {
    "reward_assigned_vehicle": 95.0,
    "penalty_skipped_vehicle": 25.0,
    "energy_satisfaction_weight": 1.1,
    "behavior": "Maximiza asignaciones, optimiza despuÃ©s",
    "use_case": "Sistemas con alta rotaciÃ³n"
}
Insights de Negocio Cuantificados:

Trade-offs precisos: "Aumentar energy_cost_weight 0.2 â†’ reduce costos 12% pero disminuye satisfacciÃ³n 8%"
Configuraciones por contexto: "Sistemas <50 EVs: usar EconomyBot, Sistemas >200 EVs: usar CriticalBot"
Sensibilidad de hiperparÃ¡metros: "learning_rate Ã³ptimo varÃ­a por arquetipo: 0.001 para Economy, 0.003 para Premium"


ğŸ“Š MÃ‰TRICAS DE EVALUACIÃ“N Y ANÃLISIS
FunciÃ³n de Fitness Principal:
pythondef fitness_function(hyperparameters, systems_subset, episodes):
    """
    FITNESS = Promedio de recompensas totales
    (La recompensa ya incluye satisfacciÃ³n + costos + eficiencia)
    """
    total_rewards = []
    
    for system_config in systems_subset:
        env = EVChargingEnv(system_config)
        env.update_reward_weights(extract_reward_weights(hyperparameters))
        
        agent = EnhancedDQNAgent(**extract_dqn_params(hyperparameters))
        results = train_dqn_agent(agent, env, episodes)
        
        avg_reward = np.mean([ep['reward'] for ep in results[-10:]])
        total_rewards.append(avg_reward)
    
    return np.mean(total_rewards)
MÃ©tricas Secundarias para AnÃ¡lisis:
pythondetailed_metrics = {
    "performance": {
        "avg_satisfaction_pct": float,
        "avg_energy_cost": float, 
        "avg_assignment_ratio": float,
        "convergence_speed": int  # episodios para converger
    },
    "stability": {
        "reward_variance": float,
        "epsilon_final": float,
        "training_stability": float
    },
    "diversity": {
        "archetype_classification": str,
        "behavior_uniqueness": float,
        "parameter_distance_to_others": float
    }
}

ğŸ”§ CONFIGURACIONES YAML COMPLETAS
configs/scatter_search_params.yaml:
yaml# Scatter Search Algorithm Configuration
algorithm:
  population_size: 150
  ref_set_size: 10
  max_iterations: 8
  max_time_hours: 48
  
  # RefSet composition
  elite_count: 6      # b1 - best quality
  diverse_count: 4    # b2 - diverse solutions
  
  # Combination parameters
  combination_methods: ["weighted_average", "geometric_mean", "random_selection"]
  combination_probability: 0.7
  alpha_range: [0.2, 0.8]  # For interpolation
  
  # Local improvement
  improvement_probability: 0.5
  perturbation_strength: 0.05  # 5% of parameter range
  max_improvement_iterations: 3

# Evaluation strategy
evaluation:
  fast:
    episodes: 30
    systems: [1, 5]
    time_budget_hours: 12
    
  medium:
    episodes: 100
    systems: [1, 3, 5, 7]
    time_budget_hours: 24
    
  full:
    episodes: 200
    systems: "all"
    time_budget_hours: 12

# Parallelization
computation:
  max_workers: 8
  gpu_allocation: "auto"
  memory_limit_gb: 16
  
# Diversity control
diversity:
  min_distance_threshold: 0.15
  archetype_representation: true
  force_archetype_minimum: 2  # Min individuals per archetype

# Logging and output
output:
  save_frequency: "every_iteration"
  checkpoint_dir: "./scatter_search_checkpoints"
  results_dir: "./scatter_search_results"
  detailed_logging: true
configs/hyperparameter_ranges.yaml:
yaml# Complete hyperparameter search space definition

dqn_hyperparameters:
  learning_rate: 
    min: 0.0001
    max: 0.005
    type: float
    description: "Adam optimizer learning rate"
    
  gamma: 
    min: 0.95
    max: 0.999
    type: float
    description: "Discount factor for future rewards"
    
  epsilon_start: 
    min: 0.8
    max: 1.0
    type: float
    description: "Initial exploration rate"
    
  epsilon_min: 
    min: 0.01
    max: 0.05
    type: float
    description: "Minimum exploration rate"
    
  epsilon_decay: 
    min: 0.995
    max: 0.999
    type: float
    description: "Exploration decay rate"
    
  batch_size: 
    values: [32, 64, 128, 256]
    type: int_discrete
    description: "Training batch size"
    
  target_update_freq: 
    values: [10, 20, 30, 50, 100]
    type: int_discrete
    description: "Target network update frequency"
    
  memory_size: 
    values: [10000, 20000, 50000]
    type: int_discrete
    description: "Experience replay buffer size"
    
  dueling_network: 
    values: [True, False]
    type: bool
    description: "Use Dueling DQN architecture"

reward_weights:
  energy_satisfaction_weight: 
    min: 0.3
    max: 3.0
    type: float
    current_baseline: 1.0
    description: "Weight for energy satisfaction in reward function"
    
  energy_cost_weight: 
    min: 0.01
    max: 1.0
    type: float
    current_baseline: 0.1
    description: "Weight for energy cost in reward function"
    
  penalty_skipped_vehicle: 
    min: 10.0
    max: 200.0
    type: float
    current_baseline: 50.0
    description: "Penalty for vehicles not assigned to charging"
    
  reward_assigned_vehicle: 
    min: 1.0
    max: 100.0
    type: float
    current_baseline: 20.0
    description: "Reward for successfully assigning vehicles"

# Archetype definitions for diversity control
archetypes:
  cost_minimizer:
    energy_cost_weight: [0.6, 1.0]
    energy_satisfaction_weight: [0.3, 0.8]
    description: "Prioritizes cost reduction over satisfaction"
    
  satisfaction_maximizer:
    energy_satisfaction_weight: [2.0, 3.0]
    energy_cost_weight: [0.01, 0.2]
    description: "Prioritizes complete energy satisfaction"
    
  balanced_optimizer:
    energy_satisfaction_weight: [1.0, 1.8]
    energy_cost_weight: [0.2, 0.6]
    description: "Balanced approach to cost and satisfaction"
    
  urgency_focused:
    penalty_skipped_vehicle: [120, 200]
    reward_assigned_vehicle: [60, 100]
    description: "Never leaves vehicles unattended"
    
  efficiency_focused:
    reward_assigned_vehicle: [70, 100]
    penalty_skipped_vehicle: [10, 40]
    description: "Maximizes throughput and assignments"

ğŸ’» ARQUITECTURA DE IMPLEMENTACIÃ“N DETALLADA
src/optimization/scatter_search.py (Clase Principal):
pythonclass ScatterSearchOptimizer:
    """
    ImplementaciÃ³n completa de Scatter Search para optimizaciÃ³n 
    de hiperparÃ¡metros DQN + pesos de recompensa
    """
    
    def __init__(self, config_path):
        self.load_configuration(config_path)
        self.setup_evaluation_systems()
        self.initialize_hyperparameter_space()
        self.setup_logging()
        
    def run_optimization(self):
        """Algoritmo principal de Scatter Search"""
        # Fase 1: PoblaciÃ³n inicial
        population = self.generate_initial_population()
        
        # Fase 2: RefSet inicial  
        refset = self.build_reference_set(population)
        
        # Fase 3: Iteraciones principales
        for iteration in range(self.max_iterations):
            # CombinaciÃ³n
            new_solutions = self.combination_phase(refset)
            
            # Mejora local
            improved_solutions = self.improvement_phase(new_solutions)
            
            # ActualizaciÃ³n RefSet
            refset = self.update_reference_set(refset, improved_solutions)
            
            # Logging y checkpoints
            self.save_iteration_results(iteration, refset)
            
        # Fase 4: EvaluaciÃ³n final
        final_candidates = self.final_evaluation(refset)
        
        return self.analyze_and_return_results(final_candidates)
Flujo Completo de EjecuciÃ³n:
python# main.py execution flow
if __name__ == "__main__":
    if args.mode == "optimize_hyperparameters":
        
        # 1. Cargar sistemas de prueba
        systems = load_all_test_systems("./data")
        
        # 2. Inicializar optimizador
        optimizer = ScatterSearchOptimizer(
            config_path="./configs/scatter_search_params.yaml",
            systems=systems
        )
        
        # 3. Ejecutar optimizaciÃ³n (48 horas)
        results = optimizer.run_optimization()
        
        # 4. AnÃ¡lizar y guardar resultados
        for archetype, config in results.items():
            print(f"\n{archetype.upper()}:")
            print(f"  Fitness: {config['fitness']:.2f}")
            print(f"  Config: {config['hyperparameters']}")
            
            # Guardar configuraciÃ³n para uso futuro
            save_path = f"./configs/optimized_{archetype}.yaml"
            save_optimized_config(config, save_path)
            
        # 5. Generar visualizaciones
        generate_optimization_visualizations(results)

ğŸ¯ VALOR AGREGADO Y DIFERENCIACIÃ“N
Innovaciones Clave:

OptimizaciÃ³n SimultÃ¡nea DQN + Reward Function: Primer enfoque que optimiza tanto hiperparÃ¡metros de aprendizaje como funciÃ³n objetivo
Arquetipos de Comportamiento: Produce mÃºltiples agentes especializados en lugar de uno solo
EvaluaciÃ³n Multi-Nivel: Estrategia computacionalmente eficiente para 48 horas de optimizaciÃ³n
Diversidad Garantizada: Asegura representaciÃ³n de diferentes estrategias de negocio
Sistema de ProducciÃ³n: Configuraciones exportables para uso inmediato

Aplicabilidad Comercial:

EconomyBot: Estaciones pÃºblicas con presiÃ³n de costos
PremiumBot: Hoteles, centros comerciales premium
CriticalBot: Hospitales, aeropuertos, servicios emergencia
BalancedBot: Oficinas corporativas, usage general
EfficiencyBot: Flotas comerciales, alta rotaciÃ³n


âœ… CRITERIOS DE Ã‰XITO
Objetivos Cuantitativos:

Mejora sobre baseline: â‰¥15% en fitness promedio
Diversidad de arquetipos: MÃ­nimo 5 comportamientos Ãºnicos
Robustez: Rendimiento consistente en â‰¥80% de sistemas
Eficiencia computacional: Completar en â‰¤48 horas

Objetivos Cualitativos:

Interpretabilidad: Cada arquetipo claramente diferenciado
Aplicabilidad: Configuraciones directamente utilizables
Escalabilidad: Sistema extensible a nuevos parÃ¡metros
Reproducibilidad: Resultados consistentes con semillas fijas


ğŸš€ PLAN DE IMPLEMENTACIÃ“N
Secuencia de Desarrollo:

Implementar hyperparameter_space.py (DefiniciÃ³n espacio bÃºsqueda)
Implementar evaluation_strategies.py (EvaluaciÃ³n multi-nivel)
Implementar scatter_search.py (Algoritmo principal)
Modificar archivos existentes (environment.py, training.py, main.py)
Crear configuraciones YAML (ParÃ¡metros y rangos)
Testing y validaciÃ³n (Pruebas en sistemas pequeÃ±os)
EjecuciÃ³n completa (48 horas optimizaciÃ³n)
AnÃ¡lisis y documentaciÃ³n (Resultados y insights)

Dependencias y Consideraciones:

Hardware: GPU(s) potente(s) con memoria suficiente
ParalelizaciÃ³n: Configurar mÃºltiples workers eficientemente
Almacenamiento: Sistema de checkpoints robusto
Monitoreo: Dashboard tiempo real del progreso
Backup: Estrategia de respaldo ante fallos


ğŸ¯ PREGUNTA FINAL DE CONFIRMACIÃ“N
Â¿EstÃ¡s conforme con esta especificaciÃ³n completa? Incluye:

âœ… PoblaciÃ³n 150, RefSet 10 con justificaciÃ³n temporal
âœ… 13 parÃ¡metros total (9 DQN + 4 reward weights) con rangos refinados
âœ… EvaluaciÃ³n multi-nivel para viabilidad computacional
âœ… Diversidad de arquetipos garantizada en RefSet
âœ… Timeline detallado para 48 horas de optimizaciÃ³n
âœ… Arquitectura de implementaciÃ³n completa y modular
âœ… Configuraciones YAML completamente especificadas
âœ… Resultados esperados con mÃºltiples agentes especializados

Si estÃ¡s de acuerdo, comenzamos la implementaciÃ³n con scatter_search.py y los archivos de configuraciÃ³n. ğŸš€